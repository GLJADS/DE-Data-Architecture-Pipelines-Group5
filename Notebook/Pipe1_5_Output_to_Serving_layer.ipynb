{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e820de0-6b1d-4ff3-9cbc-c3aed54354d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session started.\n",
      "\n",
      "DONE: data loaded.\n",
      "Written: fact_daily_content_metrics\n",
      "Written: fact_user_engagement\n",
      "Written: dim_user_segments\n",
      "Written: churn_risk_scores\n",
      "Written CSV: daily_top10_content\n",
      "Written CSV: engagement_summary_by_segment\n",
      "Written CSV: churn_summary_daily\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import avg, col, count, desc\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "\n",
    "# =========================================\n",
    "# START SPARK SESSION\n",
    "# =========================================\n",
    "\n",
    "# Configuration\n",
    "project_id = os.environ.get(\"PROJECT_ID\", \"dejadsgl\")\n",
    "bq_dataset = os.environ.get(\"BQ_DATASET\", \"netflix\")\n",
    "temp_bucket = os.environ.get(\"TEMP_BUCKET\", \"netflix-group5-temp_gl\")\n",
    "gcs_data_bucket = os.environ.get(\"GCS_DATA_BUCKET\", \"netflix_data_25\")\n",
    "\n",
    "\n",
    "# Spark configuration\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(os.getenv(\"SPARK_MASTER\", \"local[*]\"))\n",
    "sparkConf.setAppName(\"OutputToServingLayer\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# Create the Spark session\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector\n",
    "spark.conf.set('temporaryGcsBucket', temp_bucket)\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "print(\"Spark session started.\")\n",
    "\n",
    "# =========================================\n",
    "# LOAD TABLES\n",
    "# =========================================\n",
    "\n",
    "# Load data from BigQuery\n",
    "df = spark.read \\\n",
    "            .format(\"bigquery\") \\\n",
    "            .load(f\"{project_id}.{bq_dataset}.unified_review_dataset\")\n",
    "\n",
    "print(\"\\nDONE: data loaded.\")\n",
    "\n",
    "# ============================================\n",
    "# BASIS: events + datumkolom op basis van review_date\n",
    "# ============================================\n",
    "\n",
    "events = (\n",
    "    df\n",
    "    # review_date -> event_date\n",
    "    .withColumn(\"event_date\", F.to_date(\"review_date\"))\n",
    ")\n",
    "\n",
    "# Helper-kolommen op basis van sentiment / verificatie\n",
    "events = (\n",
    "    events\n",
    "    .withColumn(\n",
    "        \"is_positive\",\n",
    "        F.when(F.col(\"sentiment\") == \"positive\", F.lit(1)).otherwise(F.lit(0))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"is_negative\",\n",
    "        F.when(F.col(\"sentiment\") == \"negative\", F.lit(1)).otherwise(F.lit(0))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"is_verified_flag\",\n",
    "        F.when(F.col(\"is_verified_watch\") == True, F.lit(1)).otherwise(F.lit(0))\n",
    "    )\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 1. fact_daily_content_metrics (partitioned by event_date)\n",
    "#    Per dag x movie: review- en rating-metrics\n",
    "# ============================================\n",
    "\n",
    "fact_daily_content_metrics = (\n",
    "    events\n",
    "    .groupBy(\n",
    "        \"event_date\",\n",
    "        \"movie_id\"\n",
    "    )\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_reviews\"),\n",
    "        F.sum(\"is_verified_flag\").alias(\"verified_reviews\"),\n",
    "        F.avg(\"rating\").alias(\"avg_rating\"),\n",
    "        F.avg(\"sentiment_score\").alias(\"avg_sentiment_score\"),\n",
    "        F.sum(\"helpful_votes\").alias(\"total_helpful_votes\"),\n",
    "        F.sum(\"total_votes\").alias(\"total_votes\"),\n",
    "        F.avg(\"is_positive\").alias(\"positive_review_ratio\"),\n",
    "        F.avg(\"is_negative\").alias(\"negative_review_ratio\")\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    fact_daily_content_metrics\n",
    "    .write\n",
    "    .format(\"bigquery\")\n",
    "    .option(\"table\", f\"{project_id}.{bq_dataset}.fact_daily_content_metrics\")\n",
    "    .option(\"partitionField\", \"event_date\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save()\n",
    ")\n",
    "\n",
    "print(\"Written: fact_daily_content_metrics\")\n",
    "\n",
    "# ============================================\n",
    "# 2. fact_user_engagement (partitioned by event_date)\n",
    "#    Per dag x user: review- en rating-activiteit\n",
    "# ============================================\n",
    "\n",
    "fact_user_engagement = (\n",
    "    events\n",
    "    .groupBy(\n",
    "        \"event_date\",\n",
    "        \"user_id\"\n",
    "    )\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"reviews_count\"),\n",
    "        F.countDistinct(\"movie_id\").alias(\"distinct_titles_reviewed\"),\n",
    "        F.sum(\"is_verified_flag\").alias(\"verified_reviews\"),\n",
    "        F.avg(\"rating\").alias(\"avg_rating\"),\n",
    "        F.avg(\"sentiment_score\").alias(\"avg_sentiment_score\"),\n",
    "        F.sum(\"helpful_votes\").alias(\"total_helpful_votes\"),\n",
    "        F.avg(\"is_positive\").alias(\"positive_review_ratio\"),\n",
    "        F.avg(\"is_negative\").alias(\"negative_review_ratio\"),\n",
    "        F.countDistinct(\"device_type\").alias(\"devices_used\")\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    fact_user_engagement\n",
    "    .write\n",
    "    .format(\"bigquery\")\n",
    "    .option(\"table\", f\"{project_id}.{bq_dataset}.fact_user_engagement\")\n",
    "    .option(\"partitionField\", \"event_date\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save()\n",
    ")\n",
    "\n",
    "print(\"Written: fact_user_engagement\")\n",
    "\n",
    "# ============================================\n",
    "# 3. dim_user_segments\n",
    "#    Simpele segmenten op basis van review-gedrag\n",
    "# ============================================\n",
    "\n",
    "user_agg = (\n",
    "    events\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_reviews\"),\n",
    "        F.countDistinct(\"movie_id\").alias(\"distinct_titles_reviewed\"),\n",
    "        F.avg(\"rating\").alias(\"avg_rating\"),\n",
    "        F.avg(\"sentiment_score\").alias(\"avg_sentiment_score\"),\n",
    "        F.avg(\"is_positive\").alias(\"positive_review_ratio\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# simpele segmentlogica op basis van aantal reviews\n",
    "dim_user_segments = (\n",
    "    user_agg\n",
    "    .withColumn(\n",
    "        \"user_segment\",\n",
    "        F.when(F.col(\"total_reviews\") >= 50, F.lit(\"heavy_reviewer\"))\n",
    "         .when(F.col(\"total_reviews\") >= 10, F.lit(\"regular_reviewer\"))\n",
    "         .when(F.col(\"total_reviews\") >= 1, F.lit(\"occasional_reviewer\"))\n",
    "         .otherwise(F.lit(\"no_reviews\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    dim_user_segments\n",
    "    .write\n",
    "    .format(\"bigquery\")\n",
    "    .option(\"table\", f\"{project_id}.{bq_dataset}.dim_user_segments\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save()\n",
    ")\n",
    "\n",
    "print(\"Written: dim_user_segments\")\n",
    "\n",
    "# ============================================\n",
    "# 4. churn_risk_scores (heuristiek op basis van recency)\n",
    "#    Hoe langer geleden de laatste review, hoe hoger churn-risico\n",
    "# ============================================\n",
    "\n",
    "# laatste review per user\n",
    "last_review_per_user = (\n",
    "    events\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(\n",
    "        F.max(\"event_date\").alias(\"last_review_date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# globale max-datum (referentiepunt)\n",
    "max_date = events.agg(F.max(\"event_date\").alias(\"max_date\")).collect()[0][\"max_date\"]\n",
    "\n",
    "# dagen sinds laatste review + simpele score 0–1\n",
    "churn_risk_scores = (\n",
    "    last_review_per_user\n",
    "    .withColumn(\n",
    "        \"days_since_last_review\",\n",
    "        F.datediff(F.lit(max_date), F.col(\"last_review_date\"))\n",
    "    )\n",
    "    # schaal naar 0–1 (bijv. 0 dagen -> 0, >= 180 dagen -> 1)\n",
    "    .withColumn(\n",
    "        \"churn_risk_score\",\n",
    "        F.when(F.col(\"days_since_last_review\") <= 0, F.lit(0.0))\n",
    "         .when(F.col(\"days_since_last_review\") >= 180, F.lit(1.0))\n",
    "         .otherwise(F.col(\"days_since_last_review\") / F.lit(180.0))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"churn_risk_bucket\",\n",
    "        F.when(F.col(\"churn_risk_score\") >= 0.8, F.lit(\"high\"))\n",
    "         .when(F.col(\"churn_risk_score\") >= 0.4, F.lit(\"medium\"))\n",
    "         .otherwise(F.lit(\"low\"))\n",
    "    )\n",
    "    .withColumnRenamed(\"last_review_date\", \"score_date\")\n",
    ")\n",
    "\n",
    "(\n",
    "    churn_risk_scores\n",
    "    .write\n",
    "    .format(\"bigquery\")\n",
    "    .option(\"table\", f\"{project_id}.{bq_dataset}.churn_risk_scores\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save()\n",
    ")\n",
    "\n",
    "print(\"Written: churn_risk_scores\")\n",
    "\n",
    "# ============================================\n",
    "# 5. GCS: Summary CSV reports voor business stakeholders\n",
    "# ============================================\n",
    "\n",
    "# 5a. Daily top-10 content op basis van reviews\n",
    "w_top = Window.partitionBy(\"event_date\").orderBy(F.col(\"total_reviews\").desc())\n",
    "\n",
    "daily_top10_content = (\n",
    "    fact_daily_content_metrics\n",
    "    .withColumn(\"rank_reviews\", F.row_number().over(w_top))\n",
    "    .where(F.col(\"rank_reviews\") <= 10)\n",
    "    .orderBy(\"event_date\", \"rank_reviews\")\n",
    ")\n",
    "\n",
    "(\n",
    "    daily_top10_content\n",
    "    .coalesce(1)  # 1 CSV per run\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", True)\n",
    "    .csv(f\"gs://{report_bucket}/daily_top10_content\")\n",
    ")\n",
    "\n",
    "print(\"Written CSV: daily_top10_content\")\n",
    "\n",
    "# 5b. Dagelijkse engagement summary per segment\n",
    "engagement_with_segment = (\n",
    "    fact_user_engagement.alias(\"f\")\n",
    "    .join(\n",
    "        dim_user_segments.select(\"user_id\", \"user_segment\").alias(\"d\"),\n",
    "        on=\"user_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "engagement_summary_by_segment = (\n",
    "    engagement_with_segment\n",
    "    .groupBy(\"event_date\", \"user_segment\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "        F.avg(\"reviews_count\").alias(\"avg_reviews_per_user\"),\n",
    "        F.avg(\"avg_rating\").alias(\"avg_rating\"),\n",
    "        F.avg(\"avg_sentiment_score\").alias(\"avg_sentiment_score\"),\n",
    "        F.avg(\"positive_review_ratio\").alias(\"avg_positive_ratio\")\n",
    "    )\n",
    "    .orderBy(\"event_date\", \"user_segment\")\n",
    ")\n",
    "\n",
    "(\n",
    "    engagement_summary_by_segment\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", True)\n",
    "    .csv(f\"gs://{report_bucket}/engagement_summary_by_segment\")\n",
    ")\n",
    "\n",
    "print(\"Written CSV: engagement_summary_by_segment\")\n",
    "\n",
    "# 5c. Churn summary report\n",
    "churn_summary_daily = (\n",
    "    churn_risk_scores\n",
    "    .groupBy(\"score_date\", \"churn_risk_bucket\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"user_id\").alias(\"users_in_bucket\"),\n",
    "        F.avg(\"churn_risk_score\").alias(\"avg_churn_risk_score\")\n",
    "    )\n",
    "    .orderBy(\"score_date\", \"churn_risk_bucket\")\n",
    ")\n",
    "\n",
    "(\n",
    "    churn_summary_daily\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", True)\n",
    "    .csv(f\"gs://{report_bucket}/churn_summary_daily\")\n",
    ")\n",
    "\n",
    "print(\"Written CSV: churn_summary_daily\")\n",
    "\n",
    "print(\"\\nDONE: Output to Serving Layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f615d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
