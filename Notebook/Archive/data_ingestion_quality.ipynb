{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ca9db-68bc-49d6-a69d-ed5e2b88c17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session started.\n",
      "Importing CSV from: gs://netflix-group5-data_gl/movies.csv\n",
      "\n",
      "Loaded table: Movies\n",
      "root\n",
      " |-- movie_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- content_type: string (nullable = true)\n",
      " |-- genre_primary: string (nullable = true)\n",
      " |-- genre_secondary: string (nullable = true)\n",
      " |-- release_year: string (nullable = true)\n",
      " |-- duration_minutes: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- country_of_origin: string (nullable = true)\n",
      " |-- imdb_rating: string (nullable = true)\n",
      " |-- production_budget: string (nullable = true)\n",
      " |-- box_office_revenue: string (nullable = true)\n",
      " |-- number_of_seasons: string (nullable = true)\n",
      " |-- number_of_episodes: string (nullable = true)\n",
      " |-- is_netflix_original: string (nullable = true)\n",
      " |-- added_to_platform: string (nullable = true)\n",
      " |-- content_warning: string (nullable = true)\n",
      "\n",
      "Importing CSV from: gs://netflix-group5-data_gl/users.csv\n",
      "\n",
      "Loaded table: Users\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state_province: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- subscription_plan: string (nullable = true)\n",
      " |-- subscription_start_date: string (nullable = true)\n",
      " |-- is_active: string (nullable = true)\n",
      " |-- monthly_spend: string (nullable = true)\n",
      " |-- primary_device: string (nullable = true)\n",
      " |-- household_size: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      "\n",
      "Importing CSV from: gs://netflix-group5-data_gl/watch_history.csv\n",
      "\n",
      "Loaded table: Watch_history\n",
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- movie_id: string (nullable = true)\n",
      " |-- watch_date: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- watch_duration_minutes: string (nullable = true)\n",
      " |-- progress_percentage: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- quality: string (nullable = true)\n",
      " |-- location_country: string (nullable = true)\n",
      " |-- is_download: string (nullable = true)\n",
      " |-- user_rating: string (nullable = true)\n",
      "\n",
      "Importing CSV from: gs://netflix-group5-data_gl/reviews.csv\n",
      "\n",
      "Loaded table: Reviews\n",
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- movie_id: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- review_date: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- is_verified_watch: string (nullable = true)\n",
      " |-- helpful_votes: string (nullable = true)\n",
      " |-- total_votes: string (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- sentiment_score: string (nullable = true)\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import avg, col, count, desc\n",
    "\n",
    "# =========================================\n",
    "# 0. START SPARK SESSION\n",
    "# =========================================\n",
    "\n",
    "# Configuration\n",
    "project_id = \"dejadsgl\"\n",
    "bq_dataset = \"netflix\"\n",
    "temp_bucket = \"netflix-group5-temp_gl\"\n",
    "data_bucket = \"netflix-group5-data_gl\"\n",
    "gcs_data_bucket = \"netflix_data_25\"\n",
    "\n",
    "# Spark configuration\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"SparkCleanDataset\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# Create the Spark session\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector\n",
    "spark.conf.set('temporaryGcsBucket', temp_bucket)\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "print(\"Spark session started.\")\n",
    "\n",
    "# =========================================\n",
    "# 1. LOAD ALL TABLES\n",
    "# =========================================\n",
    "\n",
    "# Load data from BigQuery\n",
    "tables = {}\n",
    "titles = [\n",
    "    \"movies.csv\",\n",
    "    \"users.csv\",\n",
    "    \"watch_history.csv\",\n",
    "    \"reviews.csv\"\n",
    "]\n",
    "\n",
    "for title in titles:\n",
    "    #  Google Storage File Path\n",
    "    gsc_file_path = f\"gs://{data_bucket}/{title}\" \n",
    "    print(f\"Importing CSV from: {gsc_file_path}\")\n",
    "    \n",
    "    # Create data frame\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \",\") \\\n",
    "           .load(gsc_file_path)\n",
    "    \n",
    "    df.cache()\n",
    "    \n",
    "    # update the table title name starting with capital and without .csv\n",
    "    title = title.replace(\".csv\", \"\").capitalize()\n",
    "\n",
    "    # store in dictionary\n",
    "    tables[title] = df   \n",
    "\n",
    "    print(f\"\\nLoaded table: {title}\")\n",
    "    df.printSchema()\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2fd9e7f-7166-458f-b79c-effe9da588bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Cleaning table: Movies ==========\n",
      "\n",
      "STEP 0: Starting data-cleaning pipeline...\n",
      "Initial row count: 1040\n",
      "\n",
      "STEP 1: Handling missing values\n",
      " - No numerical columns found for imputation.\n",
      " - Imputing categorical columns: ['movie_id', 'title', 'content_type', 'genre_primary', 'genre_secondary', 'release_year', 'duration_minutes', 'rating', 'language', 'country_of_origin', 'imdb_rating', 'production_budget', 'box_office_revenue', 'number_of_seasons', 'number_of_episodes', 'is_netflix_original', 'added_to_platform', 'content_warning']\n",
      "   Filled missing values in 'movie_id' with mode='movie_0823'\n",
      "   Filled missing values in 'title' with mode='A Adventure'\n",
      "   Filled missing values in 'content_type' with mode='Movie'\n",
      "   Filled missing values in 'genre_primary' with mode='Adventure'\n",
      "   Filled missing values in 'release_year' with mode='2018'\n",
      "   Filled missing values in 'duration_minutes' with mode='51.0'\n",
      "   Filled missing values in 'rating' with mode='TV-Y'\n",
      "   Filled missing values in 'language' with mode='English'\n",
      "   Filled missing values in 'country_of_origin' with mode='USA'\n",
      "   Filled missing values in 'is_netflix_original' with mode='False'\n",
      "   Filled missing values in 'added_to_platform' with mode='2023-09-28'\n",
      "   Filled missing values in 'content_warning' with mode='False'\n",
      "   Completed categorical imputation.\n",
      "   Row count after categorical imputation: 1040\n",
      "\n",
      "STEP 2: Removing duplicates\n",
      " - No suitable duplicate keys found, skipping deduplication.\n",
      "\n",
      "STEP 3: Filtering outliers using IQR method\n",
      " - No numeric columns, skipping outlier filtering.\n",
      "\n",
      "STEP 4: Pipeline completed for table: Movies\n",
      "Final schema:\n",
      "root\n",
      " |-- movie_id: string (nullable = false)\n",
      " |-- title: string (nullable = false)\n",
      " |-- content_type: string (nullable = false)\n",
      " |-- genre_primary: string (nullable = false)\n",
      " |-- genre_secondary: string (nullable = true)\n",
      " |-- release_year: string (nullable = false)\n",
      " |-- duration_minutes: string (nullable = false)\n",
      " |-- rating: string (nullable = false)\n",
      " |-- language: string (nullable = false)\n",
      " |-- country_of_origin: string (nullable = false)\n",
      " |-- imdb_rating: string (nullable = true)\n",
      " |-- production_budget: string (nullable = true)\n",
      " |-- box_office_revenue: string (nullable = true)\n",
      " |-- number_of_seasons: string (nullable = true)\n",
      " |-- number_of_episodes: string (nullable = true)\n",
      " |-- is_netflix_original: string (nullable = false)\n",
      " |-- added_to_platform: string (nullable = false)\n",
      " |-- content_warning: string (nullable = false)\n",
      "\n",
      "\n",
      "========== Cleaning table: Users ==========\n",
      "\n",
      "STEP 0: Starting data-cleaning pipeline...\n",
      "Initial row count: 10300\n",
      "\n",
      "STEP 1: Handling missing values\n",
      " - No numerical columns found for imputation.\n",
      " - Imputing categorical columns: ['user_id', 'email', 'first_name', 'last_name', 'age', 'gender', 'country', 'state_province', 'city', 'subscription_plan', 'subscription_start_date', 'is_active', 'monthly_spend', 'primary_device', 'household_size', 'created_at']\n",
      "   Filled missing values in 'user_id' with mode='user_09536'\n",
      "   Filled missing values in 'email' with mode='wadetyler@example.org'\n",
      "   Filled missing values in 'first_name' with mode='Michael'\n",
      "   Filled missing values in 'last_name' with mode='Smith'\n",
      "   Filled missing values in 'gender' with mode='Female'\n",
      "   Filled missing values in 'country' with mode='USA'\n",
      "   Filled missing values in 'state_province' with mode='North Carolina'\n",
      "   Filled missing values in 'city' with mode='North Michael'\n",
      "   Filled missing values in 'subscription_plan' with mode='Standard'\n",
      "   Filled missing values in 'subscription_start_date' with mode='2025-02-18'\n",
      "   Filled missing values in 'is_active' with mode='True'\n",
      "   Filled missing values in 'primary_device' with mode='Desktop'\n",
      "   Filled missing values in 'household_size' with mode='2.0'\n",
      "   Filled missing values in 'created_at' with mode='2024-07-24 12:44:25.884428'\n",
      "   Completed categorical imputation.\n",
      "   Row count after categorical imputation: 10300\n",
      "\n",
      "STEP 2: Removing duplicates\n",
      " - No suitable duplicate keys found, skipping deduplication.\n",
      "\n",
      "STEP 3: Filtering outliers using IQR method\n",
      " - No numeric columns, skipping outlier filtering.\n",
      "\n",
      "STEP 4: Pipeline completed for table: Users\n",
      "Final schema:\n",
      "root\n",
      " |-- user_id: string (nullable = false)\n",
      " |-- email: string (nullable = false)\n",
      " |-- first_name: string (nullable = false)\n",
      " |-- last_name: string (nullable = false)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = false)\n",
      " |-- country: string (nullable = false)\n",
      " |-- state_province: string (nullable = false)\n",
      " |-- city: string (nullable = false)\n",
      " |-- subscription_plan: string (nullable = false)\n",
      " |-- subscription_start_date: string (nullable = false)\n",
      " |-- is_active: string (nullable = false)\n",
      " |-- monthly_spend: string (nullable = true)\n",
      " |-- primary_device: string (nullable = false)\n",
      " |-- household_size: string (nullable = false)\n",
      " |-- created_at: string (nullable = false)\n",
      "\n",
      "\n",
      "========== Cleaning table: Watch_history ==========\n",
      "\n",
      "STEP 0: Starting data-cleaning pipeline...\n",
      "Initial row count: 105000\n",
      "\n",
      "STEP 1: Handling missing values\n",
      " - No numerical columns found for imputation.\n",
      " - Imputing categorical columns: ['session_id', 'user_id', 'movie_id', 'watch_date', 'device_type', 'watch_duration_minutes', 'progress_percentage', 'action', 'quality', 'location_country', 'is_download', 'user_rating']\n",
      "   Filled missing values in 'session_id' with mode='session_016880'\n",
      "   Filled missing values in 'user_id' with mode='user_06554'\n",
      "   Filled missing values in 'movie_id' with mode='movie_0939'\n",
      "   Filled missing values in 'watch_date' with mode='2024-05-01'\n",
      "   Filled missing values in 'device_type' with mode='Desktop'\n",
      "   Filled missing values in 'action' with mode='started'\n",
      "   Filled missing values in 'quality' with mode='HD'\n",
      "   Filled missing values in 'location_country' with mode='USA'\n",
      "   Filled missing values in 'is_download' with mode='False'\n",
      "   Completed categorical imputation.\n",
      "   Row count after categorical imputation: 105000\n",
      "\n",
      "STEP 2: Removing duplicates\n",
      " - No suitable duplicate keys found, skipping deduplication.\n",
      "\n",
      "STEP 3: Filtering outliers using IQR method\n",
      " - No numeric columns, skipping outlier filtering.\n",
      "\n",
      "STEP 4: Pipeline completed for table: Watch_history\n",
      "Final schema:\n",
      "root\n",
      " |-- session_id: string (nullable = false)\n",
      " |-- user_id: string (nullable = false)\n",
      " |-- movie_id: string (nullable = false)\n",
      " |-- watch_date: string (nullable = false)\n",
      " |-- device_type: string (nullable = false)\n",
      " |-- watch_duration_minutes: string (nullable = true)\n",
      " |-- progress_percentage: string (nullable = true)\n",
      " |-- action: string (nullable = false)\n",
      " |-- quality: string (nullable = false)\n",
      " |-- location_country: string (nullable = false)\n",
      " |-- is_download: string (nullable = false)\n",
      " |-- user_rating: string (nullable = true)\n",
      "\n",
      "\n",
      "========== Cleaning table: Reviews ==========\n",
      "\n",
      "STEP 0: Starting data-cleaning pipeline...\n",
      "Initial row count: 15450\n",
      "\n",
      "STEP 1: Handling missing values\n",
      " - No numerical columns found for imputation.\n",
      " - Imputing categorical columns: ['review_id', 'user_id', 'movie_id', 'rating', 'review_date', 'device_type', 'is_verified_watch', 'helpful_votes', 'total_votes', 'review_text', 'sentiment', 'sentiment_score']\n",
      "   Filled missing values in 'review_id' with mode='review_014859'\n",
      "   Filled missing values in 'user_id' with mode='user_05784'\n",
      "   Filled missing values in 'movie_id' with mode='movie_0317'\n",
      "   Filled missing values in 'rating' with mode='4'\n",
      "   Filled missing values in 'review_date' with mode='2025-12-31'\n",
      "   Filled missing values in 'device_type' with mode='Mobile'\n",
      "   Filled missing values in 'is_verified_watch' with mode='True'\n",
      "   Filled missing values in 'helpful_votes' with mode='3.0'\n",
      "   Filled missing values in 'total_votes' with mode='5.0'\n",
      "   Filled missing values in 'review_text' with mode='This series is a masterpiece!'\n",
      "   Filled missing values in 'sentiment' with mode='positive'\n",
      "   Completed categorical imputation.\n",
      "   Row count after categorical imputation: 15450\n",
      "\n",
      "STEP 2: Removing duplicates\n",
      " - No suitable duplicate keys found, skipping deduplication.\n",
      "\n",
      "STEP 3: Filtering outliers using IQR method\n",
      " - No numeric columns, skipping outlier filtering.\n",
      "\n",
      "STEP 4: Pipeline completed for table: Reviews\n",
      "Final schema:\n",
      "root\n",
      " |-- review_id: string (nullable = false)\n",
      " |-- user_id: string (nullable = false)\n",
      " |-- movie_id: string (nullable = false)\n",
      " |-- rating: string (nullable = false)\n",
      " |-- review_date: string (nullable = false)\n",
      " |-- device_type: string (nullable = false)\n",
      " |-- is_verified_watch: string (nullable = false)\n",
      " |-- helpful_votes: string (nullable = false)\n",
      " |-- total_votes: string (nullable = false)\n",
      " |-- review_text: string (nullable = false)\n",
      " |-- sentiment: string (nullable = false)\n",
      " |-- sentiment_score: string (nullable = true)\n",
      "\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (IntegerType, LongType, FloatType, DoubleType, DecimalType, StringType)\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "\n",
    "def choose_duplicate_keys(df):\n",
    "    \"\"\"\n",
    "    Try to infer a reasonable key for dropDuplicates.\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        (\"CustomerID\", \"InvoiceDate\"),            # retail example\n",
    "        (\"user_id\", \"timestamp\"),\n",
    "        (\"userId\", \"timestamp\"),\n",
    "        (\"userId\", \"movieId\", \"timestamp\"),\n",
    "    ]\n",
    "    for keys in candidates:\n",
    "        if all(k in df.columns for k in keys):\n",
    "            return list(keys)\n",
    "    return None\n",
    "\n",
    "\n",
    "def remove_outliers_iqr(df, cols, k=1.5):\n",
    "    for c in cols:\n",
    "        print(f\"   - Processing outliers for numeric column '{c}'\")\n",
    "\n",
    "        # skip if column is all nulls\n",
    "        non_null = df.select(F.count(F.col(c))).first()[0]\n",
    "        if non_null == 0:\n",
    "            print(f\"     Skipping '{c}' (no non-null values).\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            q1, q3 = df.approxQuantile(c, [0.25, 0.75], 0.01)\n",
    "        except Exception as e:\n",
    "            print(f\"     Skipping '{c}' (approxQuantile error: {e})\")\n",
    "            continue\n",
    "\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - k * iqr\n",
    "        upper = q3 + k * iqr\n",
    "\n",
    "        before = df.count()\n",
    "        df = df.filter((F.col(c) >= lower) & (F.col(c) <= upper))\n",
    "        after = df.count()\n",
    "\n",
    "        print(f\"     Removed {before - after} outliers from '{c}'\")\n",
    "        print(f\"     New row count: {after}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_table(name, df):\n",
    "    print(f\"\\n========== Cleaning table: {name} ==========\\n\")\n",
    "    print(\"STEP 0: Starting data-cleaning pipeline...\")\n",
    "    print(\"Initial row count:\", df.count())\n",
    "\n",
    "    # Detect numeric and categorical columns for this table\n",
    "    numeric_cols = [\n",
    "        f.name for f in df.schema.fields\n",
    "        if isinstance(f.dataType, (IntegerType, LongType, FloatType, DoubleType, DecimalType))\n",
    "    ]\n",
    "    categorical_cols = [\n",
    "        f.name for f in df.schema.fields\n",
    "        if isinstance(f.dataType, StringType)\n",
    "    ]\n",
    "\n",
    "    # =========================================\n",
    "    # 1. Handling missing values (imputation)\n",
    "    # =========================================\n",
    "    print(\"\\nSTEP 1: Handling missing values\")\n",
    "\n",
    "    # a) Numerical columns\n",
    "    if numeric_cols:\n",
    "        print(\" - Imputing numerical columns:\", numeric_cols)\n",
    "\n",
    "        imputer = Imputer(\n",
    "            inputCols=numeric_cols,\n",
    "            outputCols=[c + \"_imputed\" for c in numeric_cols]\n",
    "        ).setStrategy(\"median\")\n",
    "\n",
    "        df = imputer.fit(df).transform(df)\n",
    "\n",
    "        for c in numeric_cols:\n",
    "            df = df.drop(c).withColumnRenamed(c + \"_imputed\", c)\n",
    "\n",
    "        print(\"   Completed numerical imputation.\")\n",
    "        print(\"   Row count after numeric imputation:\", df.count())\n",
    "    else:\n",
    "        print(\" - No numerical columns found for imputation.\")\n",
    "\n",
    "    # b) Categorical columns\n",
    "    if categorical_cols:\n",
    "        print(\" - Imputing categorical columns:\", categorical_cols)\n",
    "\n",
    "        for c in categorical_cols:\n",
    "            mode_row = (\n",
    "                df.groupBy(c)\n",
    "                  .count()\n",
    "                  .orderBy(F.desc(\"count\"))\n",
    "                  .first()\n",
    "            )\n",
    "            mode_value = mode_row[0] if mode_row else None\n",
    "            if mode_value is not None:\n",
    "                df = df.fillna({c: mode_value})\n",
    "                print(f\"   Filled missing values in '{c}' with mode='{mode_value}'\")\n",
    "\n",
    "        print(\"   Completed categorical imputation.\")\n",
    "        print(\"   Row count after categorical imputation:\", df.count())\n",
    "    else:\n",
    "        print(\" - No categorical (string) columns found for imputation.\")\n",
    "\n",
    "    # =========================================\n",
    "    # 2. Removing duplicates\n",
    "    # =========================================\n",
    "    print(\"\\nSTEP 2: Removing duplicates\")\n",
    "\n",
    "    dup_keys = choose_duplicate_keys(df)\n",
    "    if dup_keys:\n",
    "        print(f\" - Using keys for duplicates: {dup_keys}\")\n",
    "        before = df.count()\n",
    "        df = df.dropDuplicates(dup_keys)\n",
    "        after = df.count()\n",
    "        print(f\"   Removed {before - after} duplicates\")\n",
    "        print(\"   Row count after deduplication:\", after)\n",
    "    else:\n",
    "        print(\" - No suitable duplicate keys found, skipping deduplication.\")\n",
    "\n",
    "    # =========================================\n",
    "    # 3. Outlier filtering\n",
    "    # =========================================\n",
    "    print(\"\\nSTEP 3: Filtering outliers using IQR method\")\n",
    "\n",
    "    if numeric_cols:\n",
    "        df = remove_outliers_iqr(df, numeric_cols)\n",
    "    else:\n",
    "        print(\" - No numeric columns, skipping outlier filtering.\")\n",
    "\n",
    "    # =========================================\n",
    "    # 4. Final output\n",
    "    # =========================================\n",
    "    print(\"\\nSTEP 4: Pipeline completed for table:\", name)\n",
    "    print(\"Final schema:\")\n",
    "    df.printSchema()\n",
    "\n",
    "    # print(\"\\nFinal preview:\")\n",
    "    # df.show(10, truncate=False)\n",
    "    return df\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Run cleaning for all opened tables in `tables`\n",
    "# ---------------------------------------------------\n",
    "cleaned_tables = {}\n",
    "for table_name, table_df in tables.items():\n",
    "    cleaned_tables[table_name] = clean_table(table_name, table_df)\n",
    "print(\"Done.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "763fa958-ea34-4099-9276-2c77fc8576f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Movies' to BigQuery table dejadsgl:netflix.movies_cleaned ...\n",
      "Loaded table: movies_cleaned\n",
      "root\n",
      " |-- movie_id: string (nullable = false)\n",
      " |-- title: string (nullable = false)\n",
      " |-- content_type: string (nullable = false)\n",
      " |-- genre_primary: string (nullable = false)\n",
      " |-- genre_secondary: string (nullable = true)\n",
      " |-- release_year: string (nullable = false)\n",
      " |-- duration_minutes: string (nullable = false)\n",
      " |-- rating: string (nullable = false)\n",
      " |-- language: string (nullable = false)\n",
      " |-- country_of_origin: string (nullable = false)\n",
      " |-- imdb_rating: string (nullable = true)\n",
      " |-- production_budget: string (nullable = true)\n",
      " |-- box_office_revenue: string (nullable = true)\n",
      " |-- number_of_seasons: string (nullable = true)\n",
      " |-- number_of_episodes: string (nullable = true)\n",
      " |-- is_netflix_original: string (nullable = false)\n",
      " |-- added_to_platform: string (nullable = false)\n",
      " |-- content_warning: string (nullable = false)\n",
      "\n",
      "------------------------------------------------------------\n",
      "Writing 'Users' to BigQuery table dejadsgl:netflix.users_cleaned ...\n",
      "Loaded table: users_cleaned\n",
      "root\n",
      " |-- user_id: string (nullable = false)\n",
      " |-- email: string (nullable = false)\n",
      " |-- first_name: string (nullable = false)\n",
      " |-- last_name: string (nullable = false)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = false)\n",
      " |-- country: string (nullable = false)\n",
      " |-- state_province: string (nullable = false)\n",
      " |-- city: string (nullable = false)\n",
      " |-- subscription_plan: string (nullable = false)\n",
      " |-- subscription_start_date: string (nullable = false)\n",
      " |-- is_active: string (nullable = false)\n",
      " |-- monthly_spend: string (nullable = true)\n",
      " |-- primary_device: string (nullable = false)\n",
      " |-- household_size: string (nullable = false)\n",
      " |-- created_at: string (nullable = false)\n",
      "\n",
      "------------------------------------------------------------\n",
      "Writing 'Watch_history' to BigQuery table dejadsgl:netflix.watch_history_cleaned ...\n",
      "Loaded table: watch_history_cleaned\n",
      "root\n",
      " |-- session_id: string (nullable = false)\n",
      " |-- user_id: string (nullable = false)\n",
      " |-- movie_id: string (nullable = false)\n",
      " |-- watch_date: string (nullable = false)\n",
      " |-- device_type: string (nullable = false)\n",
      " |-- watch_duration_minutes: string (nullable = true)\n",
      " |-- progress_percentage: string (nullable = true)\n",
      " |-- action: string (nullable = false)\n",
      " |-- quality: string (nullable = false)\n",
      " |-- location_country: string (nullable = false)\n",
      " |-- is_download: string (nullable = false)\n",
      " |-- user_rating: string (nullable = true)\n",
      "\n",
      "------------------------------------------------------------\n",
      "Writing 'Reviews' to BigQuery table dejadsgl:netflix.reviews_cleaned ...\n",
      "Loaded table: reviews_cleaned\n",
      "root\n",
      " |-- review_id: string (nullable = false)\n",
      " |-- user_id: string (nullable = false)\n",
      " |-- movie_id: string (nullable = false)\n",
      " |-- rating: string (nullable = false)\n",
      " |-- review_date: string (nullable = false)\n",
      " |-- device_type: string (nullable = false)\n",
      " |-- is_verified_watch: string (nullable = false)\n",
      " |-- helpful_votes: string (nullable = false)\n",
      " |-- total_votes: string (nullable = false)\n",
      " |-- review_text: string (nullable = false)\n",
      " |-- sentiment: string (nullable = false)\n",
      " |-- sentiment_score: string (nullable = true)\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "# Run cleaning for all opened tables in `tables`\n",
    "# ---------------------------------------------------\n",
    "for cleaned_table, df in cleaned_tables.items():\n",
    "    bq_table_name = cleaned_table.lower() + \"_cleaned\"\n",
    "    full_table_id = f\"{project_id}:{bq_dataset}.{bq_table_name}\"  # dejadsgl:netflix.movies_cleaned\n",
    "\n",
    "    print(f\"Writing '{cleaned_table}' to BigQuery table {full_table_id} ...\")\n",
    "\n",
    "    (\n",
    "        df.write\n",
    "          .format(\"bigquery\")\n",
    "          .option(\"table\", full_table_id)\n",
    "          .mode(\"overwrite\")\n",
    "          .save()\n",
    "    )\n",
    "\n",
    "    print(f\"Loaded table: {bq_table_name}\")\n",
    "    df.printSchema()\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12111a76-1cff-4dcf-8e8a-30c4168161a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DE2025_flask-helloworld",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
