{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca989f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import avg, col, count, desc\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# =========================================\n",
    "# START SPARK SESSION\n",
    "# =========================================\n",
    "\n",
    "# Configuration\n",
    "project_id = \"dejadsgl\"\n",
    "bq_dataset = \"netflix\"\n",
    "temp_bucket = \"netflix-group5-temp_gl\"\n",
    "gcs_data_bucket = \"netflix_data_25\"\n",
    "\n",
    "# Spark configuration\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"AggregationsAnalytics\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# Create the Spark session\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector\n",
    "spark.conf.set('temporaryGcsBucket', temp_bucket)\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "print(\"Spark session started.\")\n",
    "\n",
    "# =========================================\n",
    "# LOAD TABLES\n",
    "# =========================================\n",
    "\n",
    "# Load data from BigQuery\n",
    "df = spark.read \\\n",
    "            .format(\"bigquery\") \\\n",
    "            .load(f\"{project_id}.{bq_dataset}.unified_review_dataset\")\n",
    "print(f\"\\nLoaded table: unified_review_dataset\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"Done.\")\n",
    "# print the top 10 of the unified_review_dataset loaded in df\n",
    "df.show(10, truncate=False)\n",
    "# Zorg dat we date-velden hebben\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"day\", F.to_date(\"review_date\"))                # datum\n",
    "    .withColumn(\"week_start\", F.date_trunc(\"week\", \"day\"))      # begin van de week\n",
    ")\n",
    "\n",
    "# =============================================================\n",
    "# DAILY TOP 10 MOVIES\n",
    "# =============================================================\n",
    "daily_views = (\n",
    "    df\n",
    "    .groupBy(\"day\", \"movie_id\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"num_events\"),          # aantal reviews / events\n",
    "        F.avg(\"rating\").alias(\"avg_rating\")        # optioneel: gemiddelde rating\n",
    "    )\n",
    ")\n",
    "\n",
    "w_day = Window.partitionBy(\"day\").orderBy(F.desc(\"num_events\"))\n",
    "\n",
    "daily_top10_per_movie = (\n",
    "    daily_views\n",
    "    .withColumn(\"rank\", F.row_number().over(w_day))\n",
    "    .filter(F.col(\"rank\") <= 10)\n",
    ")\n",
    "\n",
    "# =============================================================\n",
    "# WEEKLY TOP 10 MOVIES\n",
    "# =============================================================\n",
    "weekly_views = (\n",
    "    df\n",
    "    .groupBy(\"week_start\", \"movie_id\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"num_events\"),\n",
    "        F.avg(\"rating\").alias(\"avg_rating\")\n",
    "    )\n",
    ")\n",
    "\n",
    "w_week = Window.partitionBy(\"week_start\").orderBy(F.desc(\"num_events\"))\n",
    "\n",
    "weekly_top10_per_movie = (\n",
    "    weekly_views\n",
    "    .withColumn(\"rank\", F.row_number().over(w_week))\n",
    "    .filter(F.col(\"rank\") <= 10)\n",
    ")\n",
    "\n",
    "# =============================================================\n",
    "# Recency per user\n",
    "# =============================================================\n",
    "user_recency = (\n",
    "    df\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(F.max(\"review_date\").alias(\"last_interaction_date\"))\n",
    "    .withColumn(\n",
    "        \"days_since_last_interaction\",\n",
    "        F.datediff(F.current_date(), \"last_interaction_date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Activiteit laatste 30 dagen\n",
    "last_30d = F.date_sub(F.current_date(), 30)\n",
    "\n",
    "user_activity_30d = (\n",
    "    df\n",
    "    .filter(F.col(\"review_date\") >= last_30d)\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"events_30d\"),\n",
    "        F.countDistinct(\"movie_id\").alias(\"unique_titles_30d\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# =============================================================\n",
    "# Basis-user set (alle unieke users uit df)\n",
    "# =============================================================\n",
    "user_base = (\n",
    "    df\n",
    "    .select(\"user_id\")\n",
    "    .distinct()\n",
    "    .join(user_recency, on=\"user_id\", how=\"left\")\n",
    "    .join(user_activity_30d, on=\"user_id\", how=\"left\")\n",
    "    .fillna({\n",
    "        \"days_since_last_interaction\": 9999,\n",
    "        \"events_30d\": 0,\n",
    "        \"unique_titles_30d\": 0\n",
    "    })\n",
    ")\n",
    "\n",
    "user_segments = (\n",
    "    user_base\n",
    "    .withColumn(\n",
    "        \"segment\",\n",
    "        F.when(F.col(\"events_30d\") >= 20, \"Power user\")\n",
    "         .when(F.col(\"days_since_last_interaction\") <= 7, \"Active\")\n",
    "         .when((F.col(\"days_since_last_interaction\") > 7) & (F.col(\"days_since_last_interaction\") <= 30), \"At-risk\")\n",
    "         .otherwise(\"Dormant\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# =============================================================\n",
    "# Regional viewing patterns\n",
    "# =============================================================\n",
    "\n",
    "regional_viewing_patterns = (\n",
    "    df\n",
    "    .groupBy(\"location_country\", \"genre_primary\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"events\"),\n",
    "        F.countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "        F.countDistinct(\"movie_id\").alias(\"unique_titles\")\n",
    "    )\n",
    ")\n",
    "\n",
    "w_region = Window.partitionBy(\"location_country\")\n",
    "\n",
    "regional_viewing_patterns = (\n",
    "    regional_viewing_patterns\n",
    "    .withColumn(\n",
    "        \"event_share_pct\",\n",
    "        100 * F.col(\"events\") / F.sum(\"events\").over(w_region)\n",
    "    )\n",
    ")\n",
    "\n",
    "# =============================================================\n",
    "# Device usage statistics\n",
    "# =============================================================\n",
    "\n",
    "device_usage_stats = (\n",
    "    df\n",
    "    .groupBy(\"device_type\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"events\"),\n",
    "        F.countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "        F.countDistinct(\"movie_id\").alias(\"unique_titles\")\n",
    "    )\n",
    ")\n",
    "\n",
    "w_all_devices = Window.rowsBetween(Window.unboundedPreceding,\n",
    "                                   Window.unboundedFollowing)\n",
    "\n",
    "device_usage_stats = (\n",
    "    device_usage_stats\n",
    "    .withColumn(\n",
    "        \"event_share_pct\",\n",
    "        100 * F.col(\"events\") / F.sum(\"events\").over(w_all_devices)\n",
    "    )\n",
    ")\n",
    "\n",
    "# =============================================================\n",
    "# Churn risk scores per user (0-100 scale)\n",
    "# =============================================================\n",
    "\n",
    "churn_risk_scores = (\n",
    "    user_base\n",
    "    # basisrisico: elke dag inactiviteit +2 punten, max 100\n",
    "    .withColumn(\n",
    "        \"base_risk\",\n",
    "        F.least(F.col(\"days_since_last_interaction\") * 2, F.lit(100))\n",
    "    )\n",
    "    # activiteitsschild: actieve users krijgen korting op risico\n",
    "    .withColumn(\n",
    "        \"activity_bonus\",\n",
    "        F.least(F.col(\"events_30d\") * 3, F.lit(40))  # max 40 punten korting\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"churn_risk_score\",\n",
    "        F.when(F.col(\"base_risk\") - F.col(\"activity_bonus\") < 0, 0)\n",
    "         .when(F.col(\"base_risk\") - F.col(\"activity_bonus\") > 100, 100)\n",
    "         .otherwise(F.col(\"base_risk\") - F.col(\"activity_bonus\"))\n",
    "    )\n",
    "    .select(\n",
    "        \"user_id\",\n",
    "        \"days_since_last_interaction\",\n",
    "        \"events_30d\",\n",
    "        \"unique_titles_30d\",\n",
    "        \"churn_risk_score\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
