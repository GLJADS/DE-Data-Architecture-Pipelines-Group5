{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6619e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import avg, col, count, desc\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (IntegerType, LongType, FloatType, DoubleType, DecimalType, StringType)\n",
    "from pyspark.ml.feature import Imputer\n",
    "import os\n",
    "\n",
    "# =========================================\n",
    "# START SPARK SESSION\n",
    "# =========================================\n",
    "\n",
    "# Configuration\n",
    "project_id = os.environ.get(\"PROJECT_ID\", \"dejadsgl\")\n",
    "bq_dataset = os.environ.get(\"BQ_DATASET\", \"netflix\")\n",
    "temp_bucket = os.environ.get(\"TEMP_BUCKET\", \"netflix-group5-temp_gl\")\n",
    "gcs_data_bucket = os.environ.get(\"GCS_DATA_BUCKET\", \"netflix_data_25\")\n",
    "\n",
    "# Spark configuration\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(os.getenv(\"SPARK_MASTER\", \"local[*]\"))\n",
    "sparkConf.setAppName(\"CleanDataset\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# Create the Spark session\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector\n",
    "spark.conf.set('temporaryGcsBucket', temp_bucket)\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "print(\"Spark session started.\")\n",
    "\n",
    "# =========================================\n",
    "# LOAD TABLES\n",
    "# =========================================\n",
    "\n",
    "# Load data from BigQuery\n",
    "tables = {}\n",
    "titles = [\n",
    "    \"movies.csv\",\n",
    "    \"users.csv\",\n",
    "    \"watch_history.csv\",\n",
    "    \"reviews.csv\"\n",
    "]\n",
    "\n",
    "for title in titles:\n",
    "    #  Google Storage File Path\n",
    "    gsc_file_path = f\"gs://{gcs_data_bucket}/{title}\" \n",
    "    print(f\"Importing CSV from: {gsc_file_path}\")\n",
    "    \n",
    "    # Create data frame\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \",\") \\\n",
    "           .load(gsc_file_path)\n",
    "    \n",
    "    df.cache()\n",
    "    \n",
    "    # update the table title name starting with capital and without .csv\n",
    "    title = title.replace(\".csv\", \"\").capitalize()\n",
    "\n",
    "    # store in dictionary\n",
    "    tables[title] = df   \n",
    "\n",
    "    print(f\"\\nLoaded table: {title}\")\n",
    "    df.printSchema()\n",
    "\n",
    "print(\"DONE: loading tables from CSV.\")\n",
    "\n",
    "# =========================================\n",
    "# HELPERS\n",
    "# =========================================\n",
    "\n",
    "def choose_duplicate_keys(df):\n",
    "    \"\"\"\n",
    "    Try to infer a reasonable key for dropDuplicates.\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        (\"CustomerID\", \"InvoiceDate\"),            # retail example\n",
    "        (\"user_id\", \"timestamp\"),\n",
    "        (\"userId\", \"timestamp\"),\n",
    "        (\"userId\", \"movieId\", \"timestamp\"),\n",
    "    ]\n",
    "    for keys in candidates:\n",
    "        if all(k in df.columns for k in keys):\n",
    "            return list(keys)\n",
    "    return None\n",
    "\n",
    "\n",
    "def remove_outliers_iqr(df, cols, k=1.5):\n",
    "    for c in cols:\n",
    "        print(f\"   - Processing outliers for numeric column '{c}'\")\n",
    "\n",
    "        # skip if column is all nulls\n",
    "        non_null = df.select(F.count(F.col(c))).first()[0]\n",
    "        if non_null == 0:\n",
    "            print(f\"     Skipping '{c}' (no non-null values).\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            q1, q3 = df.approxQuantile(c, [0.25, 0.75], 0.01)\n",
    "        except Exception as e:\n",
    "            print(f\"     Skipping '{c}' (approxQuantile error: {e})\")\n",
    "            continue\n",
    "\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - k * iqr\n",
    "        upper = q3 + k * iqr\n",
    "\n",
    "        before = df.count()\n",
    "        df = df.filter((F.col(c) >= lower) & (F.col(c) <= upper))\n",
    "        after = df.count()\n",
    "\n",
    "        print(f\"     Removed {before - after} outliers from '{c}'\")\n",
    "        print(f\"     New row count: {after}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_table(name, df):\n",
    "    print(f\"\\n========== Cleaning table: {name} ==========\\n\")\n",
    "    print(\"STEP 0: Starting data-cleaning pipeline...\")\n",
    "    print(\"Initial row count:\", df.count())\n",
    "\n",
    "    # Detect numeric and categorical columns for this table\n",
    "    numeric_cols = [\n",
    "        f.name for f in df.schema.fields\n",
    "        if isinstance(f.dataType, (IntegerType, LongType, FloatType, DoubleType, DecimalType))\n",
    "    ]\n",
    "    categorical_cols = [\n",
    "        f.name for f in df.schema.fields\n",
    "        if isinstance(f.dataType, StringType)\n",
    "    ]\n",
    "\n",
    "    # =========================================\n",
    "    # Handling missing values (imputation)\n",
    "    # =========================================\n",
    "    print(\"\\nSTEP 1: Handling missing values\")\n",
    "\n",
    "    # a) Numerical columns\n",
    "    if numeric_cols:\n",
    "        print(\" - Imputing numerical columns:\", numeric_cols)\n",
    "\n",
    "        imputer = Imputer(\n",
    "            inputCols=numeric_cols,\n",
    "            outputCols=[c + \"_imputed\" for c in numeric_cols]\n",
    "        ).setStrategy(\"median\")\n",
    "\n",
    "        df = imputer.fit(df).transform(df)\n",
    "\n",
    "        for c in numeric_cols:\n",
    "            df = df.drop(c).withColumnRenamed(c + \"_imputed\", c)\n",
    "\n",
    "        print(\"   Completed numerical imputation.\")\n",
    "        print(\"   Row count after numeric imputation:\", df.count())\n",
    "    else:\n",
    "        print(\" - No numerical columns found for imputation.\")\n",
    "\n",
    "    # b) Categorical columns\n",
    "    if categorical_cols:\n",
    "        print(\" - Imputing categorical columns:\", categorical_cols)\n",
    "\n",
    "        for c in categorical_cols:\n",
    "            mode_row = (\n",
    "                df.groupBy(c)\n",
    "                  .count()\n",
    "                  .orderBy(F.desc(\"count\"))\n",
    "                  .first()\n",
    "            )\n",
    "            mode_value = mode_row[0] if mode_row else None\n",
    "            if mode_value is not None:\n",
    "                df = df.fillna({c: mode_value})\n",
    "                print(f\"   Filled missing values in '{c}' with mode='{mode_value}'\")\n",
    "\n",
    "        print(\"   Completed categorical imputation.\")\n",
    "        print(\"   Row count after categorical imputation:\", df.count())\n",
    "    else:\n",
    "        print(\" - No categorical (string) columns found for imputation.\")\n",
    "\n",
    "    # =========================================\n",
    "    # Removing duplicates\n",
    "    # =========================================\n",
    "    print(\"\\nSTEP 2: Removing duplicates\")\n",
    "\n",
    "    dup_keys = choose_duplicate_keys(df)\n",
    "    if dup_keys:\n",
    "        print(f\" - Using keys for duplicates: {dup_keys}\")\n",
    "        before = df.count()\n",
    "        df = df.dropDuplicates(dup_keys)\n",
    "        after = df.count()\n",
    "        print(f\"   Removed {before - after} duplicates\")\n",
    "        print(\"   Row count after deduplication:\", after)\n",
    "    else:\n",
    "        print(\" - No suitable duplicate keys found, skipping deduplication.\")\n",
    "\n",
    "    # =========================================\n",
    "    # Outlier filtering\n",
    "    # =========================================\n",
    "    print(\"\\nSTEP 3: Filtering outliers using IQR method\")\n",
    "\n",
    "    if numeric_cols:\n",
    "        df = remove_outliers_iqr(df, numeric_cols)\n",
    "    else:\n",
    "        print(\" - No numeric columns, skipping outlier filtering.\")\n",
    "\n",
    "    # =========================================\n",
    "    # Final output\n",
    "    # =========================================\n",
    "    print(\"\\nSTEP 4: Pipeline completed for table:\", name)\n",
    "    print(\"Final schema:\")\n",
    "    df.printSchema()\n",
    "\n",
    "    # print(\"\\nFinal preview:\")\n",
    "    # df.show(10, truncate=False)\n",
    "    return df\n",
    "\n",
    "# =========================================\n",
    "# Run cleaning for all opened tables in `tables`\n",
    "# =========================================\n",
    "cleaned_tables = {}\n",
    "for table_name, table_df in tables.items():\n",
    "    cleaned_tables[table_name] = clean_table(table_name, table_df)\n",
    "print(\"DONE: cleaning tables.\\n\")\n",
    "\n",
    "# =========================================\n",
    "# Save the cleaned table to BigQuery\n",
    "# =========================================\n",
    "for cleaned_table, df in cleaned_tables.items():\n",
    "    bq_table_name = cleaned_table.lower() + \"_cleaned\"\n",
    "    full_table_id = f\"{project_id}:{bq_dataset}.{bq_table_name}\"  # dejadsgl:netflix.movies_cleaned\n",
    "\n",
    "    print(f\"Writing '{cleaned_table}' to BigQuery table {full_table_id} ...\")\n",
    "\n",
    "    (\n",
    "        df.write\n",
    "          .format(\"bigquery\")\n",
    "          .option(\"table\", full_table_id)\n",
    "          .mode(\"overwrite\")\n",
    "          .save()\n",
    "    )\n",
    "\n",
    "    print(f\"Loaded table: {bq_table_name}\")\n",
    "    df.printSchema()\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "print(\"DONE: writing tables to BigQuery.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c8884-1332-4535-81e2-511c65cf0448",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
